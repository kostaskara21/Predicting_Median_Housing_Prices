# -*- coding: utf-8 -*-
"""Predicting_Median_Housing_Prices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MNejwlEw1FcnBE_rUiAUM3wgk63OeHFP
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

data = pd.read_csv('housing.csv')

"""**Data Pre-Processing**"""

data.head()

"""As we can tell all variables are quantitative except for ocean_proximity which is categorical

**SCALING**

The mix max scaler takes the range of values ‚Äã‚Äã(max-min) and transforms each value to belong to a range, usually between 0 and 1
"""

col_names = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value','ocean_proximity']

for col in col_names[:-1]:
  data[col]=(data[col]-data[col].min())/(data[col].max()-data[col].min())

data.head()

"""**ONE HOT VECTOR ENCODING**

For the data, which are categorical variables (ocean_proximity), we will encode them with one hot vector encoding to get a vector representation..
"""

data['ocean_proximity']

"""First, we will check our data to see how many unique categories we have

"""

data['ocean_proximity'].unique()

for d in data['ocean_proximity'].unique():
  print(d)

"""As we can tell we have 5 different categories, so to our data we will add 5 columns, where each one will refer to a specific category of ocean_proximity, eg Ocean_proximity_Near_bay etc. For this we will use the get_dummies() function provided in the pandas library.




"""

df = pd.DataFrame(data)
print('Original DataFrame')
display(df)
df_encoded = pd.get_dummies(df, dtype = int)
print('\n DataFrame after performing One-hot Encoding')
df_encoded

"""**FIXING MISSING VALUES**

We will replace the missing value with the media of the each attribute

In order to find the median of one attribute :

The median is the middle value in a set of data. First, organize and order the data from smallest to largest. Divide the number of observations by two to find the midpoint value. Round the number up if there's an odd number of observations and the value in that position is the median. Take the average of the values found above and below that position if the number of observations is even.
"""

def Find_media(column_name):
    # Sorting without modifying the original list
    sorted_col = sorted(column_name)

    # Find the size of the list
    lenoflist = len(sorted_col)

    # Find the middle index
    middle_ind = lenoflist // 2

    # If the list length is even, take the average of the two middle elements
    if lenoflist % 2 == 0:
        return (sorted_col[middle_ind - 1] + sorted_col[middle_ind]) / 2
    else:
        return sorted_col[middle_ind]  # Correct return for odd-length lists

data=df_encoded

"""Now we are checking to see witch values are missing"""

data.isnull().sum()

bedrooms=[]
for d in data['total_bedrooms']:
  bedrooms.append(d)
Find_media(bedrooms)

data['total_bedrooms'].fillna(0.04989137181874612,inplace=True)

data.isnull().sum()

"""**VISUALIZATION**"""

plt.hist(data['longitude'])

plt.hist(data['latitude'])

plt.hist(data['housing_median_age'])

plt.hist(data['total_rooms'])

plt.hist(data['total_bedrooms'])

plt.hist(data['population'])

plt.hist(data['households'])

plt.hist(data['median_income'])

plt.hist(data['median_house_value'])

plt.hist(df['ocean_proximity']) #We are using thr dataframe before encoding

data['ocean_proximity_ISLAND'].sum() #The amount of the ones in that column thats the total value

"""
Now we will create 2D graphs of data that  represent combinations of 2, 3 or even 4 variables ex:"""

plt.scatter(data['population'],data['households'])

"""Looking at the graph, we csn  understand that the larger the population in the suburb, the more households"""

df1=df_encoded.iloc[:,:-5]

df1

sns.pairplot(df1)
plt.show()

correlation_matrix = df1.corr()
print(correlation_matrix)

plt.figure(figsize=(8,6))
sns.heatmap(correlation_matrix, annot=True, cmap = 'coolwarm', fmt=".2f")
plt.title('Œ†ŒπŒΩŒ±Œ∫Œ±œÇ Œ£œÖŒΩŒ¥ŒπŒ±œÉœÄŒøœÅŒ±œÇ')
plt.show()

"""From the correlation matrix, we can see that median house value is most strongly linked to median income (0.688), meaning higher-income areas tend to have more expensive houses. There is also a small negative link between latitude and house value (-0.144), which might indicate that houses in the south are more expensive. Also, total rooms, total bedrooms, population, and households are closely related, which makes sense since bigger communities have more housing and people. Lastly, older neighborhoods tend to have fewer rooms (-0.361) on average.

**Data Regression**

**Implementation of the Perceptron Algorithm (Binary Classification) for Linear Discriminant Function**

In this task, I am implementing the Perceptron Algorithm to create a linear discriminant function of the form g:  R<sup>ùëô</sup> ‚Üí {-1, +1},  where ùëô
 represents the dimension of the feature space. The objective is to train the model to classify data based on this function. To achieve this, a suitable threshold is considered, which is based on the median house value. This threshold is used to divide the continuous range of house values into two distinct sets, enabling the problem to be approached as a binary classification task. The final goal is to correctly categorize data points into these two classes using the Perceptron algorithm.
"""

values=[]
for d in data['median_house_value']:
  values.append(d)
Find_media(values)

"""If a data point's value exceeds the threshold, it is classified as -1, whereas values below or equal to the threshold are assigned to class 1."""

categories = []
for v in data['median_house_value']:
  if v > 0.33958829035756555 :
    categories.append(1)
  else:
    categories.append(-1)

data['median_house_value'].head()

categories[:5]

len(categories)

df_encoded

features_for_perceptron = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','ocean_proximity_<1H OCEAN','ocean_proximity_INLAND','ocean_proximity_ISLAND','ocean_proximity_NEAR BAY','ocean_proximity_NEAR OCEAN']

X = df_encoded[features_for_perceptron].to_numpy()

X

epochs = 100

type(X)

features = X
labels = categories

def perceptron_train(features,labels):

  #Initializing our weights with 0
  w = np.zeros(features.shape[1]+1) # The weight vector w is initialized as a 1-row, 13-column vector, matching the number of features in our dataset

  for epoch in range(epochs):
        for vector, label in zip(features, labels):
            vector = np.insert(vector,0,1) #In the position 0 we assign the value  1 , f(x) = w0*1 + x1*w1 + .... + x13*w13
            y = np.dot(w, vector.transpose()) #y = f(x) = (w0,w1,...w13)*(1,x1,x2,..x13)
            if (y > 0):
              target = 1
            else:
              target = -1

            delta = (label - target)

            if delta != 0: # misclassified
                #update our w
                w += label * vector

  return w

def perceptron_test(features,labels,w):
  er=0
  for vector, label in zip(features, labels):

      vector = np.insert(vector,0,1)
      y = np.dot(w, vector.transpose()) #y = f(x) = (w0,w1,...w13)*(1,x1,x2,..x13)
      if (y > 0):
        target = 1
      else:
        target = -1

      delta = (label - target)

      if delta != 0: # misclassified
          er+=1

  #print(f"Test error count: {er}")
  return er

w = perceptron_train(X,categories)

w

"""To evaluate the performance of the trained Perceptron, we iterate through each feature vector using the learned weight vector w to check the misclassified datas"""

er=0
for vector, label in zip(features, labels):
    vector = np.insert(vector,0,1)
    y = np.dot(w, vector.transpose()) #y = f(x) = (w0,w1,...w13)*(1,x1,x2,..x13)
    if (y > 0):
      target = 1
    else:
      target = -1

    delta = (label - target)

    if delta != 0: # misclassified
      er+=1

er

errors=perceptron_test(X,categories,w)

errors

"""And we can see that the Test gives us same results"""

y= np.array(categories)

"""The data is split into 10 equal parts the model is trained on 9 folds and tested on the remaining 1 part. This process is repeated 10 times, with each fold serving as the test set once. The final performance metric is the average of the results from all 10 iterations, giving a more accurate and trustworthy estimate of how well the model performs."""

from sklearn.model_selection import KFold
k_folds = KFold(n_splits=10, shuffle=True, random_state=40) #shuffle true œÑŒ± Œ±ŒΩŒ±Œ∫Œ±œÑŒµœÖŒµŒπ

sum_errors = 0
for train_index, val_index in k_folds.split(X):
    X_train, y_train  = X[train_index],y[train_index]
    X_val, y_val = X[val_index], y[val_index]
    w = perceptron_train(X_train,y_train)
    errors =  perceptron_test(X_val,y_val,w)
    sum_errors += errors/len(y_val)
    print(errors/len(y_val))
print('Average. false',sum_errors/10)

"""The average error of around 25% which indicates that the data may not follow a linear relationship.  If the data were truly linear, a model like a perceptron would have 0 error

**Implementation of the Least Squares Algorithm (Linear Regression) for Continuous Prediction**

In this task, I am implementing the Least Squares Algorithm to create a linear regression model of the form g: R<sup>ùëô</sup> ‚Üí R, where ùëô represents the dimension of the feature space.
"""

X = X[:,:-5]

"""Due to the one-hot encoding of categorical variables, the last five columns in the dataset made the  feature matrix non-invertible during the Least Squares calculation. By removing them we successfully implemented the algorithm.

"""

X.shape

ones = np.ones((X.shape[0],1))

ones.shape

X = np.concatenate((ones,X),axis=1)

X.shape

y = df1['median_house_value'].values

y

from sklearn.metrics import mean_squared_error,mean_absolute_error

sum_mse = 0
sum_mae = 0
for train_index, val_index in k_folds.split(X):
  X_train, y_train  = X[train_index],y[train_index]
  X_val, y_val = X[val_index], y[val_index]
  #lms(train - finding the w)
  XT_X = np.dot(X_train.T,X_train)
  XT_X_inverse = np.linalg.inv(XT_X)
  XT_X_inverse_XT = np.dot(XT_X_inverse,X_train.T)
  w = np.dot(XT_X_inverse_XT,y_train)
  #lms(prediction)
  y_pred = np.dot(X_val,w)
  mse = mean_squared_error(y_pred,y_val)
  sum_mse +=mse
  mae = mean_absolute_error(y_pred,y_val)
  sum_mae += mae
  print('Least squared error: ',mse,' Mean Absolute Error ',mae)

print('Mean of the Mean Squared Error: ',sum_mse/10,'Mean of the Mean Absolute Error: ',sum_mae/10)